from transformers import T5Tokenizer, T5ForConditionalGeneration
import torch

def generate_response(model, tokenizer, prompt_text):
  input_ids = tokenizer.encode(prompt_text, return_tensors="pt")
  device = input_ids.device  # ensure the model uses the same device as the input_ids
  output = model.generate(input_ids.to(device),
                          max_length=100,
                          num_return_sequences=1,
                          temperature=0.9,
                          pad_token_id=tokenizer.eos_token_id,
                          do_sample=True,
                          attention_mask=input_ids.new_ones(input_ids.shape, dtype=torch.long))
  response = tokenizer.decode(output[0], skip_special_tokens=True)
  return response

model_1 = T5ForConditionalGeneration.from_pretrained("t5-small")  # small-sized model
tokenizer_1 = T5Tokenizer.from_pretrained("t5-small")

model_2 = T5ForConditionalGeneration.from_pretrained("t5-base")  # medium-sized model
tokenizer_2 = T5Tokenizer.from_pretrained("t5-base")

model_3 = T5ForConditionalGeneration.from_pretrained("t5-large")  # large-sized model
tokenizer_3 = T5Tokenizer.from_pretrained("t5-large")

questions = [
  "Provide a concise definition of euthanasia."
]

for i, question in enumerate(questions):
  print(f"QUESTION {i+1}: {question}")

  response_1 = generate_response(model_1, tokenizer_1, question)
  response_2 = generate_response(model_2, tokenizer_2, question)
  response_3 = generate_response(model_3, tokenizer_3, question)

  print("\n - Model 1 (Basic) answer to", response_1)
  print("\n - Model 2 (Medium) answer to", response_2)
  print("\n - Model 3 (Large) answer to", response_3)
  print("\n")
